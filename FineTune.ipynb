{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from torch.utils.data import Dataset,DataLoader,random_split\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch import optim\r\n",
    "from transformers import BertTokenizer,BertModel\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_df = pd.read_csv('data/train_dataset.csv',usecols=['text','label'])\r\n",
    "print(train_df.shape)\r\n",
    "train_df.head()\r\n",
    "sentences = list(train_df['text'])\r\n",
    "labels =train_df['label'].values"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5000, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def flat_accuracy(preds,labels):\r\n",
    "    pred_flat=np.argmax(preds,axis=1).flatten()\r\n",
    "    labels_flat=labels.flatten()\r\n",
    "    return accuracy_score(labels_flat,pred_flat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class DataToDataset(Dataset):\r\n",
    "    def __init__(self,sentences,labels):\r\n",
    "        tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
    "        max_length=600\r\n",
    "        self.encoding=tokenizer(sentences,padding=True,truncation=True,max_length=max_length,return_tensors='pt')\r\n",
    "        self.labels=torch.tensor(labels)\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.labels)\r\n",
    "        \r\n",
    "    def __getitem__(self,index):\r\n",
    "        return self.encoding['input_ids'][index],self.encoding['attention_mask'][index],self.labels[index]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "datasets=DataToDataset(sentences,labels)\r\n",
    "train_size=int(len(datasets)*0.8)\r\n",
    "test_size=len(datasets)-train_size\r\n",
    "train_dataset,val_dataset=random_split(dataset=datasets,lengths=[train_size,test_size])\r\n",
    "BATCH_SIZE=32\r\n",
    "train_loader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=True)\r\n",
    "val_loader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class BertTextClassficationModel(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(BertTextClassficationModel,self).__init__()\r\n",
    "        self.bert=BertModel.from_pretrained('bert-base-uncased')\r\n",
    "        self.classification_head=nn.Sequential(nn.Linear(768,200),nn.ReLU(inplace=True),nn.Linear(200,2),nn.ReLU(inplace=True))\r\n",
    "        \r\n",
    "    def forward(self,ids,mask):\r\n",
    "        out=self.bert(input_ids=ids,attention_mask=mask).last_hidden_state\r\n",
    "        out=self.classification_head(out[:,0,:])\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "loss_func=nn.CrossEntropyLoss()\r\n",
    "model=BertTextClassficationModel()\r\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.0001)\r\n",
    "device=\"cpu\"\r\n",
    "writer=SummaryWriter('./logs')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "epochs=10\r\n",
    "for epoch in range(epochs):\r\n",
    "    train_loss = 0.0\r\n",
    "    train_acc=0.0\r\n",
    "    for i,data in enumerate(train_loader):\r\n",
    "        print(epoch,i)\r\n",
    "        input_ids,attention_mask,labels=[elem.to(device) for elem in data]\r\n",
    "        #优化器置零\r\n",
    "        optimizer.zero_grad()\r\n",
    "        #得到模型的结果\r\n",
    "        out=model(input_ids,attention_mask)\r\n",
    "        #计算误差\r\n",
    "        loss=loss_func(out,labels)\r\n",
    "        writer.add_scalar('loss',loss,epoch)\r\n",
    "        train_loss += loss.item()\r\n",
    "        #误差反向传播\r\n",
    "        loss.backward()\r\n",
    "        #更新模型参数\r\n",
    "        optimizer.step()\r\n",
    "        #计算acc \r\n",
    "        out=out.detach().numpy()\r\n",
    "        labels=labels.detach().numpy()\r\n",
    "        train_acc+=flat_accuracy(out,labels)\r\n",
    "\r\n",
    "    print(\"train %d/%d epochs Loss:%f, Acc:%f\" %(epoch,epochs,train_loss/(i+1),train_acc/(i+1)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "0 9\n",
      "0 10\n",
      "0 11\n",
      "0 12\n",
      "0 13\n",
      "0 14\n",
      "0 15\n",
      "0 16\n",
      "0 17\n",
      "0 18\n",
      "0 19\n",
      "0 20\n",
      "0 21\n",
      "0 22\n",
      "0 23\n",
      "0 24\n",
      "0 25\n",
      "0 26\n",
      "0 27\n",
      "0 28\n",
      "0 29\n",
      "0 30\n",
      "0 31\n",
      "0 32\n",
      "0 33\n",
      "0 34\n",
      "0 35\n",
      "0 36\n",
      "0 37\n",
      "0 38\n",
      "0 39\n",
      "0 40\n",
      "0 41\n",
      "0 42\n",
      "0 43\n",
      "0 44\n",
      "0 45\n",
      "0 46\n",
      "0 47\n",
      "0 48\n",
      "0 49\n",
      "0 50\n",
      "0 51\n",
      "0 52\n",
      "0 53\n",
      "0 54\n",
      "0 55\n",
      "0 56\n",
      "0 57\n",
      "0 58\n",
      "0 59\n",
      "0 60\n",
      "0 61\n",
      "0 62\n",
      "0 63\n",
      "0 64\n",
      "0 65\n",
      "0 66\n",
      "0 67\n",
      "0 68\n",
      "0 69\n",
      "0 70\n",
      "0 71\n",
      "0 72\n",
      "0 73\n",
      "0 74\n",
      "0 75\n",
      "0 76\n",
      "0 77\n",
      "0 78\n",
      "0 79\n",
      "0 80\n",
      "0 81\n",
      "0 82\n",
      "0 83\n",
      "0 84\n",
      "0 85\n",
      "0 86\n",
      "0 87\n",
      "0 88\n",
      "0 89\n",
      "0 90\n",
      "0 91\n",
      "0 92\n",
      "0 93\n",
      "0 94\n",
      "0 95\n",
      "0 96\n",
      "0 97\n",
      "0 98\n",
      "0 99\n",
      "0 100\n",
      "0 101\n",
      "0 102\n",
      "0 103\n",
      "0 104\n",
      "0 105\n",
      "0 106\n",
      "0 107\n",
      "0 108\n",
      "0 109\n",
      "0 110\n",
      "0 111\n",
      "0 112\n",
      "0 113\n",
      "0 114\n",
      "0 115\n",
      "0 116\n",
      "0 117\n",
      "0 118\n",
      "0 119\n",
      "0 120\n",
      "0 121\n",
      "0 122\n",
      "0 123\n",
      "0 124\n",
      "train 0/3 epochs Loss:0.027788, Acc:0.992000\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 7\n",
      "1 8\n",
      "1 9\n",
      "1 10\n",
      "1 11\n",
      "1 12\n",
      "1 13\n",
      "1 14\n",
      "1 15\n",
      "1 16\n",
      "1 17\n",
      "1 18\n",
      "1 19\n",
      "1 20\n",
      "1 21\n",
      "1 22\n",
      "1 23\n",
      "1 24\n",
      "1 25\n",
      "1 26\n",
      "1 27\n",
      "1 28\n",
      "1 29\n",
      "1 30\n",
      "1 31\n",
      "1 32\n",
      "1 33\n",
      "1 34\n",
      "1 35\n",
      "1 36\n",
      "1 37\n",
      "1 38\n",
      "1 39\n",
      "1 40\n",
      "1 41\n",
      "1 42\n",
      "1 43\n",
      "1 44\n",
      "1 45\n",
      "1 46\n",
      "1 47\n",
      "1 48\n",
      "1 49\n",
      "1 50\n",
      "1 51\n",
      "1 52\n",
      "1 53\n",
      "1 54\n",
      "1 55\n",
      "1 56\n",
      "1 57\n",
      "1 58\n",
      "1 59\n",
      "1 60\n",
      "1 61\n",
      "1 62\n",
      "1 63\n",
      "1 64\n",
      "1 65\n",
      "1 66\n",
      "1 67\n",
      "1 68\n",
      "1 69\n",
      "1 70\n",
      "1 71\n",
      "1 72\n",
      "1 73\n",
      "1 74\n",
      "1 75\n",
      "1 76\n",
      "1 77\n",
      "1 78\n",
      "1 79\n",
      "1 80\n",
      "1 81\n",
      "1 82\n",
      "1 83\n",
      "1 84\n",
      "1 85\n",
      "1 86\n",
      "1 87\n",
      "1 88\n",
      "1 89\n",
      "1 90\n",
      "1 91\n",
      "1 92\n",
      "1 93\n",
      "1 94\n",
      "1 95\n",
      "1 96\n",
      "1 97\n",
      "1 98\n",
      "1 99\n",
      "1 100\n",
      "1 101\n",
      "1 102\n",
      "1 103\n",
      "1 104\n",
      "1 105\n",
      "1 106\n",
      "1 107\n",
      "1 108\n",
      "1 109\n",
      "1 110\n",
      "1 111\n",
      "1 112\n",
      "1 113\n",
      "1 114\n",
      "1 115\n",
      "1 116\n",
      "1 117\n",
      "1 118\n",
      "1 119\n",
      "1 120\n",
      "1 121\n",
      "1 122\n",
      "1 123\n",
      "1 124\n",
      "train 1/3 epochs Loss:0.000509, Acc:1.000000\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "2 6\n",
      "2 7\n",
      "2 8\n",
      "2 9\n",
      "2 10\n",
      "2 11\n",
      "2 12\n",
      "2 13\n",
      "2 14\n",
      "2 15\n",
      "2 16\n",
      "2 17\n",
      "2 18\n",
      "2 19\n",
      "2 20\n",
      "2 21\n",
      "2 22\n",
      "2 23\n",
      "2 24\n",
      "2 25\n",
      "2 26\n",
      "2 27\n",
      "2 28\n",
      "2 29\n",
      "2 30\n",
      "2 31\n",
      "2 32\n",
      "2 33\n",
      "2 34\n",
      "2 35\n",
      "2 36\n",
      "2 37\n",
      "2 38\n",
      "2 39\n",
      "2 40\n",
      "2 41\n",
      "2 42\n",
      "2 43\n",
      "2 44\n",
      "2 45\n",
      "2 46\n",
      "2 47\n",
      "2 48\n",
      "2 49\n",
      "2 50\n",
      "2 51\n",
      "2 52\n",
      "2 53\n",
      "2 54\n",
      "2 55\n",
      "2 56\n",
      "2 57\n",
      "2 58\n",
      "2 59\n",
      "2 60\n",
      "2 61\n",
      "2 62\n",
      "2 63\n",
      "2 64\n",
      "2 65\n",
      "2 66\n",
      "2 67\n",
      "2 68\n",
      "2 69\n",
      "2 70\n",
      "2 71\n",
      "2 72\n",
      "2 73\n",
      "2 74\n",
      "2 75\n",
      "2 76\n",
      "2 77\n",
      "2 78\n",
      "2 79\n",
      "2 80\n",
      "2 81\n",
      "2 82\n",
      "2 83\n",
      "2 84\n",
      "2 85\n",
      "2 86\n",
      "2 87\n",
      "2 88\n",
      "2 89\n",
      "2 90\n",
      "2 91\n",
      "2 92\n",
      "2 93\n",
      "2 94\n",
      "2 95\n",
      "2 96\n",
      "2 97\n",
      "2 98\n",
      "2 99\n",
      "2 100\n",
      "2 101\n",
      "2 102\n",
      "2 103\n",
      "2 104\n",
      "2 105\n",
      "2 106\n",
      "2 107\n",
      "2 108\n",
      "2 109\n",
      "2 110\n",
      "2 111\n",
      "2 112\n",
      "2 113\n",
      "2 114\n",
      "2 115\n",
      "2 116\n",
      "2 117\n",
      "2 118\n",
      "2 119\n",
      "2 120\n",
      "2 121\n",
      "2 122\n",
      "2 123\n",
      "2 124\n",
      "train 2/3 epochs Loss:0.000186, Acc:1.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "val_loss=0\r\n",
    "val_acc=0\r\n",
    "model.eval()\r\n",
    "for j,batch in enumerate(val_loader):\r\n",
    "    val_input_ids,val_attention_mask,val_labels=[elem.to(device) for elem in batch]\r\n",
    "    with torch.no_grad():\r\n",
    "        pred=model(val_input_ids,val_attention_mask)\r\n",
    "        val_loss+=loss_func(pred,val_labels)\r\n",
    "        pred=pred.detach().cpu().numpy()\r\n",
    "        val_labels=val_labels.detach().cpu().numpy()\r\n",
    "        val_acc+=flat_accuracy(pred,val_labels)\r\n",
    "print(\"evaluate loss:%d, Acc:%d\" %(val_loss/len(val_loader),val_acc/len(val_loader)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "evaluate loss:0, Acc:1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "writer.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "4876ff34b70794a54711585a56035755ce9dd6f9a98e80662ac35fb37c287d01"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}